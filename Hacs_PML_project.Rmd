---
title: "Hacs_HAR/PML_Project"
author: "Harold Cruz-Sanchez"
date: "September 25, 2015"
output: html_document
---
# Human Activity Recognition
====================================================
## Practical Machine Learning
----------------------------------------------------
*Harold Cruz-Sanchez*

**Executive Summary:**

Base on the hypothesis that data generated by wearable-sensors could be use to find patters of human activity, a practical machine learning alghoritm (PML) was developed, trained, verificated and tested with data generated by four sensors, in order to find patters of human activity (Weight Lifting Exercises) of a subject wearing those sensors. 

The data collected contains 5 different classes of activities, (sitting-down, standing-up, standing, walking, and sitting), and was collected on 8 hours of activities of 4 healthy subjects. The raw data was processed, synchronized, code-labeled `classe`, while the participants were supervised by an experienced weight lifter.

Read more: [linked phrase](http://groupware.les.inf.puc-rio.br/har)

After downloading the data sets, a Random Forrest model was developed to reach a ** high Accuracy (0.9937)** and **low error rate (0.6%)**.

The final model was able to correctly classify all 20 cases in the test set.


```{r}
date()
sessionInfo()
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(randomForest)
library(knitr)
library(AppliedPredictiveModeling)
library(ggplot2)
library(reshape2)

```

##Data 

This dataset is licensed under the **Creative Commons license (CC BY-SA)**. The CC BY-SA license means you can remix, tweak, and build upon this work even for commercial purposes, as long as you credit the authors of the original work and you license your new creations under the identical terms we are licensing to you.

Read more: http://groupware.les.inf.puc-rio.br/har#ixzz3mcWM4mn1
The data for this project come from this source: 
[linked phrase](http://groupware.les.inf.puc-rio.br/har).

The training data for this project are available here: 
[linked phrase](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

The test data are available here: 
[linked phrase](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

Thanks a lot to the authors
*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.*


```{r}
#================================================
# INPUT DATA
# Internet CSV File
#================================================

train_data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", na.strings = c('NA','#DIV/0!',''))

test_data <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", na.strings = c('NA','#DIV/0!',''))
```

Once the train and test data sets are loaded into R, it is advisable to check for the accuracy and dimensionality of the data sets.

```{r}
dim(train_data); dim(test_data)  # data dimensions
colnames_train <- colnames(train_data)
colnames_test <- colnames(test_data)
```

Both sets seem to contain similar number of variables, in this case 160.  Analizing that amount of variables is a cumbersome and time/resources consuming activity, thus a cleaner data sets must be used.

```{r ggplot, fig.width=4, fig.height=3, message=FALSE}
plot_01<- ggplot(train_data, aes(x = classe)) + 
  geom_histogram()
print(plot_01)
dev.copy(png, file="plot_01.png", height=480, width=480)
dev.off()

```

##Cleaning data

The devices that were used for this study generate time/position dependent data, and the initial data process added extra variables, including `user_name` and others that are not important, or the weight in the general classification may be low, thus, thise are the variables that can be eliminated.

###Removing variables with NA values

Counting the number of non_NAs in each column.

```{r}
non_NAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}
```


Removing the NA values in the train data set

```{r}
# Build vector of missing data or NA columns to drop.
train_nonNA_count <- non_NAs(train_data)

train_drop_cols <- c()

for (cnt in 1:length(train_nonNA_count)) {
    if (train_nonNA_count[cnt] < nrow(train_data)) {
        train_drop_cols <- c(train_drop_cols, colnames_train[cnt])
    }
}

train_drop_cols # variables that may be eliminated

my_train_data <- train_data[,!(names(train_data) %in% train_drop_cols)]
dim(my_train_data)
```

Removing the NA values in the test data set

```{r}
# Build vector of missing data or NA columns to drop.
test_nonNA_count <- non_NAs(test_data)

test_drop_cols <- c() 

for (count in 1:length(test_nonNA_count)) {
  if (test_nonNA_count[count] < nrow(test_data)) {
    test_drop_cols <- c(test_drop_cols, colnames_test[count])
  }
}

test_drop_cols # variables that may be eliminated

my_test_data <- test_data[,!(names(test_data) %in% test_drop_cols)]
dim(my_test_data)
```

###Removing unnecessary variables

The following variables were created during the pre-processing of the raw data, and have no interest in the present model: 

1 `X` it refers tho the row line
2 'user_name` refers to the device user
3 `raw_timestamp_part_1` refers to time stamp
4 `raw_timestamp_part_2` refers to time stamp
5 `cvtd_timestamp` refers to date/time stamp
6 `new_window`
7 `num_window` 

those variables are not important for the prediction.

```{r}

# Train set.

my_train_data <- my_train_data[,8:length(colnames(my_train_data))]
dim(my_train_data)

# Test set.

my_test_data <- my_test_data[,8:length(colnames(my_test_data))]
dim(my_test_data)
```

The process of cleaning the data sets, allow the elimination of `r 160 - 53` variables, now it is possible to compare the total number and name of the variables in both data sets

```{r}
all.equal(colnames(my_train_data),colnames(my_test_data))
```

Here is it easy to find the number of mismatched columns (variables), so it is necessary to find the mismatched one 

```{r}

mismatch <- c(colnames(my_train_data) == colnames(my_test_data)) 
mismatch
```

The result list let us know that the variable in colum 53 is different in the train and test sets. Now we can recall the column 54 in both sets
```{r}
names(my_train_data)[53]; names(my_test_data)[53]
```

The variable `classe` from the train data set will be used as **reference**, while the variable `problem_id` in the test data is to be used as **predictor**.


##Predicting Models

To create the predicting model, the train data set will be randomly splited into 2 data sets. The first part (75% of all data) will be used as trainig set, while the rest (25%) will be used as validation set.

-  Model: Random Forest 
-  method = 'rf'
-  pre-processing = NULL
-  Type: Classification, Regression
-  Tuning Parameters: mtry (#Randomly Selected Predictors)

```{r}
set.seed(123)

RF_Training <- createDataPartition(my_train_data$classe, p = .75, list = FALSE)
RF_my_data_Training <- my_train_data[ RF_Training,]
RF_my_data_validating  <- my_train_data[-RF_Training,]

set.seed(123)
RF_Fit_01 <- train(RF_my_data_Training$classe ~ ., data = RF_my_data_Training,
                 method = "rf",
                 trControl = trainControl(method = "cv", number = 4),
                 verbose = FALSE)
RF_Fit_01
```

This initial model has an `Accuracy` of 99%, when the mtry = 2.

```{r}
print(RF_Fit_01$finalModel, digits=3)
```

the previous model generate a OOB estimate of  error rate: 0.63%

####Now predicting using the validation data set

```{r}
predictions <- predict(RF_Fit_01, newdata=RF_my_data_validating)
print(confusionMatrix(predictions, RF_my_data_validating$classe), digits=4)
```
 The proposed model has an `Accuracy` of 99.37% and a `P-Value` of less than 2.2e-16, thus this model may be considered *accurate*.
 
**NOTE:** Using the same algorithm, but changing the pre-processing from NULL to ["center", "scale"], decreases the `OOB estimate of  error rate` to 0.27%, while keeping the `Accuracy~ and `P-Value` in simmilar levels.

##Prediction: Testing the model on the Test data set 

The high value of the `Accuracy` and the low value of the `P-Value`, make the model aconfident model to be used in the prediction using the test data set.

```{r}
set.seed(123)
predict(RF_Fit_01, newdata=my_test_data)
print(predict(RF_Fit_01, newdata=my_test_data))
```

```
[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```

### Prediction Results: Exporting data
```{r}
answers<-as.character(predict(RF_Fit_01, newdata=my_test_data))

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(answers)
```




